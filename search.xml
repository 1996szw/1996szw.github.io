<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【毕设】数据挖掘算法之Apriori关联规则挖掘]]></title>
    <url>%2F2018%2F01%2F12%2F%E3%80%90%E6%AF%95%E8%AE%BE%E3%80%91%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%AE%97%E6%B3%95%E4%B9%8BApriori%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98%2F</url>
    <content type="text"><![CDATA[Info: Apriori关联规则算法学习 关联分析关联分析是一种在大规模数据集中寻找有趣关系的任务。这些关系可以有两种形式: 频繁项集（frequent item sets）: 经常出现在一块的物品的集合。 关联规则（associational rules）: 暗示两种物品之间可能存在很强的关系。 相关术语 关联分析（关联规则学习): 从大规模数据集中寻找物品间的隐含关系被称作 关联分析(associati analysis) 或者 关联规则学习（association rule learning） 。 下面是用一个 杂货店 例子来说明这两个概念，如下图所示: 频繁项集: {葡萄酒, 尿布, 豆奶} 就是一个频繁项集的例子。 关联规则: 尿布 -&gt; 葡萄酒 就是一个关联规则。这意味着如果顾客买了尿布，那么他很可能会买葡萄酒。 那么 频繁 的定义是什么呢？怎么样才算频繁呢？度量它们的方法有很多种，这里我们来简单的介绍下支持度和置信度。 支持度: 数据集中包含该项集的记录所占的比例。例如上图中，{豆奶} 的支持度为 4/5。{豆奶, 尿布} 的支持度为 3/5。 置信度: 针对一条诸如 {尿布} -&gt; {葡萄酒} 这样具体的关联规则来定义的。这条规则的 置信度 被定义为 支持度({尿布, 葡萄酒})/支持度({尿布})，从图中可以看出 支持度({尿布, 葡萄酒}) = 3/5，支持度({尿布}) = 4/5，所以 {尿布} -&gt; {葡萄酒} 的置信度 = 3/5 / 4/5 = 3/4 = 0.75。 支持度 和 置信度 是用来量化 关联分析 是否成功的一个方法。假设想找到支持度大于 0.8 的所有项集，应该如何去做呢？一个办法是生成一个物品所有可能组合的清单，然后对每一种组合统计它出现的频繁程度，但是当物品成千上万时，上述做法就非常非常慢了。我们需要详细分析下这种情况并讨论下 Apriori 原理，该原理会减少关联规则学习时所需的计算量。 Apriori 原理假设我们一共有 4 个商品: 商品0, 商品1, 商品2, 商品3。所有可能的情况如下:如果我们计算所有组合的支持度，也需要计算 15 次。即 2^N - 1 = 2^4 - 1 = 15。随着物品的增加，计算的次数呈指数的形式增长 … 为了降低计算次数和时间，研究人员发现了一种所谓的 Apriori 原理，即某个项集是频繁的，那么它的所有子集也是频繁的。例如，如果 {0, 1} 是频繁的，那么 {0}, {1} 也是频繁的。 该原理直观上没有什么帮助，但是如果反过来看就有用了，也就是说如果一个项集是 非频繁项集，那么它的所有超集也是非频繁项集，如下图所示: 在图中我们可以看到，已知灰色部分 {2,3} 是 非频繁项集，那么利用上面的知识，我们就可以知道 {0,2,3} {1,2,3} {0,1,2,3} 都是 非频繁的。也就是说，计算出 {2,3} 的支持度，知道它是 非频繁 的之后，就不需要再计算 {0,2,3} {1,2,3} {0,1,2,3} 的支持度，因为我们知道这些集合不会满足我们的要求。使用该原理就可以避免项集数目的指数增长，从而在合理的时间内计算出频繁项集。 Apriori 算法优缺点 123* 优点：易编码实现* 缺点：在大数据集上可能较慢* 适用数据类型：数值型 或者标准型数据。 Apriori 算法流程步骤： 123456* 收集数据：使用任意方法。* 准备数据：任何数据类型都可以，因为我们只保存集合。* 分析数据：使用任意方法。* 训练数据：使用Apiori算法来找到频繁项集。* 测试算法：不需要测试过程。* 使用算法：用于发现频繁项集以及物品之间的关联规则。 Apriori 算法的使用前面提到，关联分析的目标包括两项: 发现 频繁项集 和发现 关联规则。首先需要找到 频繁项集，然后才能发现 关联规则。Apriori 算法是发现 频繁项集 的一种方法。Apriori 算法的两个输入参数分别是最小支持度和数据集。该算法首先会生成所有单个物品的项集列表。接着扫描交易记录来查看哪些项集满足最小支持度要求，那些不满足最小支持度要求的集合会被去掉。然后对生下来的集合进行组合以生成包含两个元素的项集。接下来再重新扫描交易记录，去掉不满足最小支持度的项集。该过程重复进行直到所有项集被去掉。 生成候选项集下面会创建一个用于构建初始集合的函数，也会创建一个通过扫描数据集以寻找交易记录子集的函数，数据扫描的伪代码如下: 对数据集中的每条交易记录 tran 对每个候选项集 can 检查一下 can 是否是 tran 的子集: 如果是则增加 can 的计数值 对每个候选项集 can 如果其支持度不低于最小值，则保留该项集 返回所有频繁项集列表 以下是一些辅助函数。 加载数据集123# 加载数据集def loadDataSet(): return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]] 创建集合 C1。即对 dataSet 进行去重，排序，放入 list 中，然后转换所有的元素为 frozenset1234567891011121314151617181920212223# 创建集合 C1。即对 dataSet 进行去重，排序，放入 list 中，然后转换所有的元素为 frozensetdef createC1(dataSet): """createC1（创建集合 C1） Args: dataSet 原始数据集 Returns: frozenset 返回一个 frozenset 格式的 list """ C1 = [] for transaction in dataSet: for item in transaction: if not [item] in C1: # 遍历所有的元素，如果不在 C1 出现过，那么就 append C1.append([item]) # 对数组进行 `从小到大` 的排序 print 'sort 前=', C1 C1.sort() # frozenset 表示冻结的 set 集合，元素无改变；可以把它当字典的 key 来使用 print 'sort 后=', C1 print 'frozenset=', map(frozenset, C1) return map(frozenset, C1) 计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于最小支持度（minSupport）的数据1234567891011121314151617181920212223242526272829303132333435# 计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于最小支持度（minSupport）的数据def scanD(D, Ck, minSupport): """scanD（计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于最小支持度 minSupport 的数据） Args: D 数据集 Ck 候选项集列表 minSupport 最小支持度 Returns: retList 支持度大于 minSupport 的集合 supportData 候选项集支持度数据 """ # ssCnt 临时存放选数据集 Ck 的频率. 例如: a-&gt;10, b-&gt;5, c-&gt;8 ssCnt = &#123;&#125; for tid in D: for can in Ck: # s.issubset(t) 测试是否 s 中的每一个元素都在 t 中 if can.issubset(tid): if not ssCnt.has_key(can): ssCnt[can] = 1 else: ssCnt[can] += 1 numItems = float(len(D)) # 数据集 D 的数量 retList = [] supportData = &#123;&#125; for key in ssCnt: # 支持度 = 候选项（key）出现的次数 / 所有数据集的数量 support = ssCnt[key]/numItems if support &gt;= minSupport: # 在 retList 的首位插入元素，只存储支持度满足频繁项集的值 retList.insert(0, key) # 存储所有的候选项（key）和对应的支持度（support） supportData[key] = support return retList, supportData 完整代码地址: https://github.com/apachecn/MachineLearning/blob/master/src/py2.x/11.Apriori/apriori.py 组织完整的 Apriori 算法输入频繁项集列表 Lk 与返回的元素个数 k，然后输出所有可能的候选项集 Ck12345678910111213141516171819202122232425262728293031# 输入频繁项集列表 Lk 与返回的元素个数 k，然后输出所有可能的候选项集 Ckdef aprioriGen(Lk, k): """aprioriGen（输入频繁项集列表 Lk 与返回的元素个数 k，然后输出候选项集 Ck。 例如: 以 &#123;0&#125;,&#123;1&#125;,&#123;2&#125; 为输入且 k = 2 则输出 &#123;0,1&#125;, &#123;0,2&#125;, &#123;1,2&#125;. 以 &#123;0,1&#125;,&#123;0,2&#125;,&#123;1,2&#125; 为输入且 k = 3 则输出 &#123;0,1,2&#125; 仅需要计算一次，不需要将所有的结果计算出来，然后进行去重操作 这是一个更高效的算法） Args: Lk 频繁项集列表 k 返回的项集元素个数（若元素的前 k-2 相同，就进行合并） Returns: retList 元素两两合并的数据集 """ retList = [] lenLk = len(Lk) for i in range(lenLk): for j in range(i+1, lenLk): L1 = list(Lk[i])[: k-2] L2 = list(Lk[j])[: k-2] # print '-----i=', i, k-2, Lk, Lk[i], list(Lk[i])[: k-2] # print '-----j=', j, k-2, Lk, Lk[j], list(Lk[j])[: k-2] L1.sort() L2.sort() # 第一次 L1,L2 为空，元素直接进行合并，返回元素两两合并的数据集 # if first k-2 elements are equal if L1 == L2: # set union # print 'union=', Lk[i] | Lk[j], Lk[i], Lk[j] retList.append(Lk[i] | Lk[j]) return retList 找出数据集 dataSet 中支持度 &gt;= 最小支持度的候选项集以及它们的支持度。即我们的频繁项集。1234567891011121314151617181920212223242526272829303132333435363738394041# 找出数据集 dataSet 中支持度 &gt;= 最小支持度的候选项集以及它们的支持度。即我们的频繁项集。def apriori(dataSet, minSupport=0.5): """apriori（首先构建集合 C1，然后扫描数据集来判断这些只有一个元素的项集是否满足最小支持度的要求。那么满足最小支持度要求的项集构成集合 L1。然后 L1 中的元素相互组合成 C2，C2 再进一步过滤变成 L2，然后以此类推，知道 CN 的长度为 0 时结束，即可找出所有频繁项集的支持度。） Args: dataSet 原始数据集 minSupport 支持度的阈值 Returns: L 频繁项集的全集 supportData 所有元素和支持度的全集 """ # C1 即对 dataSet 进行去重，排序，放入 list 中，然后转换所有的元素为 frozenset C1 = createC1(dataSet) # 对每一行进行 set 转换，然后存放到集合中 D = map(set, dataSet) print 'D=', D # 计算候选数据集 C1 在数据集 D 中的支持度，并返回支持度大于 minSupport 的数据 L1, supportData = scanD(D, C1, minSupport) # print "L1=", L1, "\n", "outcome: ", supportData # L 加了一层 list, L 一共 2 层 list L = [L1] k = 2 # 判断 L 的第 k-2 项的数据长度是否 &gt; 0。第一次执行时 L 为 [[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]]。L[k-2]=L[0]=[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]，最后面 k += 1 while (len(L[k-2]) &gt; 0): print 'k=', k, L, L[k-2] Ck = aprioriGen(L[k-2], k) # 例如: 以 &#123;0&#125;,&#123;1&#125;,&#123;2&#125; 为输入且 k = 2 则输出 &#123;0,1&#125;, &#123;0,2&#125;, &#123;1,2&#125;. 以 &#123;0,1&#125;,&#123;0,2&#125;,&#123;1,2&#125; 为输入且 k = 3 则输出 &#123;0,1,2&#125; print 'Ck', Ck Lk, supK = scanD(D, Ck, minSupport) # 计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于 minSupport 的数据 # 保存所有候选项集的支持度，如果字典没有，就追加元素，如果有，就更新元素 supportData.update(supK) if len(Lk) == 0: break # Lk 表示满足频繁子项的集合，L 元素在增加，例如: # l=[[set(1), set(2), set(3)]] # l=[[set(1), set(2), set(3)], [set(1, 2), set(2, 3)]] L.append(Lk) k += 1 # print 'k=', k, len(L[k-2]) return L, supportData 到这一步，我们就找出我们所需要的 频繁项集 和他们的 支持度 了，接下来再找出关联规则即可！ 完整代码地址: https://github.com/apachecn/MachineLearning/blob/master/src/py2.x/11.Apriori/apriori.py 从频繁项集中挖掘关联规则前面我们介绍了用于发现 频繁项集 的 Apriori 算法，现在要解决的问题是如何找出 关联规则。 要找到 关联规则，我们首先从一个 频繁项集 开始。我们知道集合中的元素是不重复的，但我们想知道基于这些元素能否获得其它内容。某个元素或某个元素集合可能会推导出另一个元素。从先前 杂货店 的例子可以得到，如果有一个频繁项集 {豆奶,莴苣}，那么就可能有一条关联规则 “豆奶 -&gt; 莴苣”。这意味着如果有人买了豆奶，那么在统计上他会购买莴苣的概率比较大。但是，这一条件反过来并不总是成立。也就是说 “豆奶 -&gt; 莴苣” 统计上显著，那么 “莴苣 -&gt; 豆奶” 也不一定成立。 前面我们给出了 频繁项集 的量化定义，即它满足最小支持度要求。对于 关联规则，我们也有类似的量化方法，这种量化指标称之为 置信度。一条规则 A -&gt; B 的置信度定义为 support(A | B) / support(A)。（注意: 在 python 中 | 表示集合的并操作，而数学书集合并的符号是 U）。A | B 是指所有出现在集合 A 或者集合 B 中的元素。由于我们先前已经计算出所有 频繁项集 的支持度了，现在我们要做的只不过是提取这些数据做一次除法运算即可。 一个频繁项集可以产生多少条关联规则呢？如下图所示，给出的是项集 {0,1,2,3} 产生的所有关联规则: 与我们前面的 频繁项集 生成一样，我们可以为每个频繁项集产生许多关联规则。如果能减少规则的数目来确保问题的可解析，那么计算起来就会好很多。通过观察，我们可以知道，如果某条规则并不满足 最小置信度 要求，那么该规则的所有子集也不会满足 最小置信度 的要求。如上图所示，假设 123 -&gt; 3 并不满足最小置信度要求，那么就知道任何左部为 {0,1,2} 子集的规则也不会满足 最小置信度 的要求。即 12 -&gt; 03 , 02 -&gt; 13 , 01 -&gt; 23 , 2 -&gt; 013, 1 -&gt; 023, 0 -&gt; 123 都不满足 最小置信度 要求。 可以利用关联规则的上述性质属性来减少需要测试的规则数目，跟先前 Apriori 算法的套路一样。以下是一些辅助函数: 计算置信度1234567891011121314151617181920212223242526# 计算置信度（confidence）def calcConf(freqSet, H, supportData, brl, minConf=0.7): """calcConf（对两个元素的频繁项，计算置信度，例如： &#123;1,2&#125;/&#123;1&#125; 或者 &#123;1,2&#125;/&#123;2&#125; 看是否满足条件） Args: freqSet 频繁项集中的元素，例如: frozenset([1, 3]) H 频繁项集中的元素的集合，例如: [frozenset([1]), frozenset([3])] supportData 所有元素的支持度的字典 brl 关联规则列表的空数组 minConf 最小置信度 Returns: prunedH 记录 置信度大于阈值的集合 """ # 记录置信度大于最小置信度（minConf）的集合 prunedH = [] for conseq in H: # 假设 freqSet = frozenset([1, 3]), H = [frozenset([1]), frozenset([3])]，那么现在需要求出 frozenset([1]) -&gt; frozenset([3]) 的置信度和 frozenset([3]) -&gt; frozenset([1]) 的置信度 # print 'confData=', freqSet, H, conseq, freqSet-conseq conf = supportData[freqSet]/supportData[freqSet-conseq] # 支持度定义: a -&gt; b = support(a | b) / support(a). 假设 freqSet = frozenset([1, 3]), conseq = [frozenset([1])]，那么 frozenset([1]) 至 frozenset([3]) 的置信度为 = support(a | b) / support(a) = supportData[freqSet]/supportData[freqSet-conseq] = supportData[frozenset([1, 3])] / supportData[frozenset([1])] if conf &gt;= minConf: # 只要买了 freqSet-conseq 集合，一定会买 conseq 集合（freqSet-conseq 集合和 conseq 集合是全集） print freqSet-conseq, '--&gt;', conseq, 'conf:', conf brl.append((freqSet-conseq, conseq, conf)) prunedH.append(conseq) return prunedH 递归计算频繁项集的规则1234567891011121314151617181920212223242526272829303132# 递归计算频繁项集的规则def rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7): """rulesFromConseq Args: freqSet 频繁项集中的元素，例如: frozenset([2, 3, 5]) H 频繁项集中的元素的集合，例如: [frozenset([2]), frozenset([3]), frozenset([5])] supportData 所有元素的支持度的字典 brl 关联规则列表的数组 minConf 最小置信度 """ # H[0] 是 freqSet 的元素组合的第一个元素，并且 H 中所有元素的长度都一样，长度由 aprioriGen(H, m+1) 这里的 m + 1 来控制 # 该函数递归时，H[0] 的长度从 1 开始增长 1 2 3 ... # 假设 freqSet = frozenset([2, 3, 5]), H = [frozenset([2]), frozenset([3]), frozenset([5])] # 那么 m = len(H[0]) 的递归的值依次为 1 2 # 在 m = 2 时, 跳出该递归。假设再递归一次，那么 H[0] = frozenset([2, 3, 5])，freqSet = frozenset([2, 3, 5]) ，没必要再计算 freqSet 与 H[0] 的关联规则了。 m = len(H[0]) if (len(freqSet) &gt; (m + 1)): print 'freqSet******************', len(freqSet), m + 1, freqSet, H, H[0] # 生成 m+1 个长度的所有可能的 H 中的组合，假设 H = [frozenset([2]), frozenset([3]), frozenset([5])] # 第一次递归调用时生成 [frozenset([2, 3]), frozenset([2, 5]), frozenset([3, 5])] # 第二次 。。。没有第二次，递归条件判断时已经退出了 Hmp1 = aprioriGen(H, m+1) # 返回置信度大于最小置信度的集合 Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf) print 'Hmp1=', Hmp1 print 'len(Hmp1)=', len(Hmp1), 'len(freqSet)=', len(freqSet) # 计算置信度后，还有数据大于最小置信度的话，那么继续递归调用，否则跳出递归 if (len(Hmp1) &gt; 1): print '----------------------', Hmp1 # print len(freqSet), len(Hmp1[0]) + 1 rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf) 生成关联规则12345678910111213141516171819202122232425# 生成关联规则def generateRules(L, supportData, minConf=0.7): """generateRules Args: L 频繁项集列表 supportData 频繁项集支持度的字典 minConf 最小置信度 Returns: bigRuleList 置信度规则列表（关于 (A-&gt;B+置信度) 3个字段的组合） """ bigRuleList = [] # 假设 L = [[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])], [frozenset([1, 3]), frozenset([2, 5]), frozenset([2, 3]), frozenset([3, 5])], [frozenset([2, 3, 5])]] for i in range(1, len(L)): # 获取频繁项集中每个组合的所有元素 for freqSet in L[i]: # 假设：freqSet= frozenset([1, 3]), H1=[frozenset([1]), frozenset([3])] # 组合总的元素并遍历子元素，并转化为 frozenset 集合，再存放到 list 列表中 H1 = [frozenset([item]) for item in freqSet] # 2 个的组合，走 else, 2 个以上的组合，走 if if (i &gt; 1): rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf) else: calcConf(freqSet, H1, supportData, bigRuleList, minConf) return bigRuleList 到这里为止，通过调用 generateRules 函数即可得出我们所需的 关联规则。 分级法： 频繁项集-&gt;关联规则 1.首先从一个频繁项集开始，接着创建一个规则列表，其中规则右部分只包含一个元素，然后对这个规则进行测试。 2.接下来合并所有剩余规则来创建一个新的规则列表，其中规则右部包含两个元素。 如下图： 最后： 每次增加频繁项集的大小，Apriori 算法都会重新扫描整个数据集，是否有优化空间呢？ 信息参考于 ApacheCN]]></content>
      <categories>
        <category>study</category>
      </categories>
      <tags>
        <tag>毕业设计</tag>
        <tag>graduation</tag>
        <tag>数据挖掘</tag>
        <tag>关联规则算法</tag>
        <tag>apriori</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【毕设】Hadoop完全分布式平台搭建]]></title>
    <url>%2F2018%2F01%2F07%2F%E3%80%90%E6%AF%95%E8%AE%BE%E3%80%91Hadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Info: 毕业设计课题基础Hadoop环境搭建 步入大四，毕设也是今年的重中之重，学校给我分配的课题是《基于大数据平台的关联规则算法的研究与实现》。之前没有接触过大数据的我也是要从头开始相关的学习，比如Hadoop环境的配置、关联规则算法的python实现、还有如何将Apriori算法在MapReduce上面并行化等等等等。本文就是Hadoop完全分布式平台的搭建记录。 另外也要感谢我校嵌入式研究中心的主任李超老师，给我提供了性能可靠的物理服务器来进行该课题的研究。 搭建Hadoop全分布式集群前提虚拟机软件的安装网上随便找的Vmware workstation，然后在centOS官网找个镜像装一下就OK了。但由于自己是学校给的服务器，所以略过这一步。 虚拟机的网络如果是在一台虚拟机中安装多个Linux操作系统的话，可以使用NAT或桥接模式都是可以的。 步骤： 在要使用的虚拟机的标签上右键单击，选择设置，选择网络适配器，选择桥接模式，确定。 设置完成之后，重启一下虚拟机。 再设置桥接之前将固定的IP取消： 桌面版：通过图形化界面设置的。 服务器版：在/etc/network/interfacesiface ens33 inet dhcp #address ... ifconfig获取IP。 最后试一试能不能ping通。 文件读写权限在linux下，软件安装到/opt下，当前正在使用的用户，对于opt目录需要有读写权限： 1）将opt的权限给为777（漏洞文件），不推荐在生产环境中使用。但是可以在学习和调试环境中使用。 2）sudo 在启动Hadoop的各个守护进程的时候，需要使用sudo。 在管理Hadoop的时候，实际上由不同的用户启动不同集群的守护进程。 统一使用当前的用户管理所有集群。 3）该目录的所有者设置为当前用户 安装JDK，配置Java环境1）将jdk安装包放在家目录下 2）解压到opt目录下 sudo tar zxvf jdk-8u131-linux-x64.tar.gz -C /opt 此时在/opt目录下：会有一个jdk1.8.0_1313）创建软链接 sudo ln -snf /opt/jdk1.8.0_131 /opt/jdk 注意：创建软连接的目的是为了，我们在做项目的时候，可能会用到不同的jdk版本，这是要换软件的话，只需要修改软链接就可以了。而不用修改配置文件。 4）配置环境变量 局部环境变量：~/.bashrc 全局环境变量：/etc/profile export JAVA_HOME=/opt/jdk export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib export PATH=$PATH:$JAVA_HOME/bin source 相关文件（更新配置文件）5）查看是否安装成功 java、javac、java -version搭建伪分布式集群安装Hadoop1）解压hadoop安装包到opt目录下 sudo tar zxvf hadoop-2.8.1.tar.gz -C /opt2）创建软链接 ln -snf /opt/hadoop-2.8.1 /opt/hadoop 3）配置环境变量 在/etc/profile文件中加入以下内容： export HADOOP_HOME=/opt/hadoop export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin source /etc/profile4）使用hadoop version命令测试是否配置成功 配置Hadoop配置文件存放在/opt/hadoop/etc/hadoop中有很多个文件，暂时只需要修改的只有几个 1）hadoop-env.sh export JAVA_HOME=${JAVA_HOME}改成export JAVA_HOME=/opt/jdk 注意：在配置文件中有提示我们怎么设置，我们一般不删除，二回选择注释它的提示。 2）core-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt; &lt;value&gt;hdfs:&#x2F;&#x2F;1.0.0.5:9000&lt;&#x2F;value&gt; &lt;&#x2F;property&gt;&lt;&#x2F;configuration&gt; 分析：1.0.0.5是你主节点所在主机的ip，而9000为端口 3）hdfs-site.xml 1234567891011121314151617181920212223&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;&#x2F;name&gt; &lt;value&gt;hadoop-cluster&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt; &lt;value&gt;file:&#x2F;&#x2F;&#x2F;data&#x2F;hadoop&#x2F;hdfs&#x2F;nn&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.ch eckpoint.dir&lt;&#x2F;name&gt; &lt;value&gt;file:&#x2F;&#x2F;&#x2F;data&#x2F;hadoop&#x2F;hdfs&#x2F;snn&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.edits.dir&lt;&#x2F;name&gt; &lt;value&gt;file:&#x2F;&#x2F;&#x2F;data&#x2F;hadoop&#x2F;hdfs&#x2F;snn&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt; &lt;value&gt;file:&#x2F;&#x2F;&#x2F;data&#x2F;hadoop&#x2F;hdfs&#x2F;dn&lt;&#x2F;value&gt; &lt;&#x2F;property&gt;&lt;&#x2F;configuration&gt; 4）mapred-site.xml 在hadoop的相关目录中没有此文件，但是有一个mapred-site.xml.template文件，将该文件复制一份为mapred-site.xml cp mapred-site.xml.template mapred-site.xml123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt; &lt;value&gt;yarn&lt;&#x2F;value&gt; &lt;&#x2F;property&gt;&lt;&#x2F;configuration&gt; 5）yarn-site.xml 12345678910111213141516&lt;configuration&gt; &lt;!-- 指定ResourceManager的地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt; &lt;value&gt;1.0.0.5&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 指定reducer获取数据的方式--&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt; &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.local-dirs&lt;&#x2F;name&gt; &lt;value&gt;file:&#x2F;&#x2F;&#x2F;data&#x2F;hadoop&#x2F;yarn&#x2F;nm&lt;&#x2F;value&gt; &lt;&#x2F;property&gt;&lt;&#x2F;configuration&gt; Hadoop目录权限问题注意： 如果使用sudo启动hadoop的相关进程，这几个目录的权限可以不用管。如果是使用当前的用户启动相关进程，对于opt目录，当前用户得有读写权限，对于/data目录也需要读写权限。 sudo mkdir -p /data/hadoop/hdfs/nn sudo mkdir -p /data/hadoop/hdfs/dn sudo mkdir -p /data/hadoop/hdfs/snn sudo mkdir -p /data/hadoop/yarn/nm启动Hadoop启动HDFS集群 hadoop-daemon.sh start namenode 启动主节点 hadoop-daemon.sh start datanode 启动从节点 启动YARN集群yarn-daemon.sh start resourcemanager yarn-daemon.sh start nodemanager 启动作业历史服务器mr-jobhistory-daemon.sh start historyserver jps命令查看是否启动成功 HDFS和YARN集群都有相对应的WEB监控页面HDFS：http://ip:50070 YARN：http://ip:8088 YARN集群的操作—-测试Hadoop是否配置成功计算PI值的作业： yarn jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar pi 4 100 配置免密码登录1）在所有的主从节点中执行 如果以前配置过免密登录的话，建议删除重新建立过，因为我们需要配置的是多台服务器： rm -r ~/.ssh 执行ssh-keygen为了在主节点中生成公钥和私钥，在从从节点生成.ssh目录 2）在主节点中执行 scp /.ssh/id_rsa.pub 从节点的用户名@从节点ip: 注意：第一次远程连接的话，首先输入yes，然后是从节点密码 3）在所有的从节点中执行 我们把主节点的公钥已经拿到了所有的从节点中，接下来就是： cat id_rsa.pub&gt;&gt;.ssh/authorized_keys当中]]></content>
      <categories>
        <category>study</category>
      </categories>
      <tags>
        <tag>毕业设计</tag>
        <tag>graduation</tag>
        <tag>hadoop</tag>
        <tag>环境配置</tag>
      </tags>
  </entry>
</search>
